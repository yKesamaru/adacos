# AdaCos損失: 両手をあげてオススメできない理由

![](assets/eye_catch.png)

## はじめに
皆さん、損失関数は何を使ってらっしゃいますか？

### ハイパーパラメーターが嫌い
経験と勘がすべてを支配する「ハイパーパラメーター指定」。これは、機械学習において、避けて通りたい作業です。ハイパーパラメーターの調整は、外せばモデルの訓練が不安定になるだけでなく、最終的な認識性能にも悪影響を与える可能性があります。

### ハイパーパラメーターフリー、かつ高性能ときいて
AdaCosはハイパーパラメーターフリーで、訓練過程で自動的にスケールパラメーターを調整できるだけでなく、高い顔認識精度を達成することが可能とききました。

そんな美味しい話があるんですね。

そこで早速、AdaCosを実装してtrainを回してみました。
しかし、なかなか精度が上がりません。どうしてだ。

そんな時は、論文を読むしかありません。

## 結論
下のグラフを見てください。
![](assets/2023-09-23-22-46-46.png)
このグラフを見ると、AdaCosにおいて、同じクラスのコサイン類似度は大きく、逆に異なるクラスのコサイン類似度がグラフ中もっとも小さいことがわかります。

注目すべきは横軸です。
ArcFaceよりAdaCosが優れた値を示すのは、2万epoch以降です。
いや、横軸がepochとは書いてません。ミニバッチかもしれない。それでもエグい数字です。

これだったらArcFaceの方がお手軽ではないかと思いました。

というわけで解散！…でもいいんですけど、AdaCosの仕組みを理解しておきたいので、以降かんたんにまとめました。

## 疑問
- 固定スケールのAdaCosの、イテレーションにおける縦軸がどこにも書いてない。
![](assets/2023-09-24-10-12-49.png)

- 固定スケールファクターならば、ArcFaceと同じepoch数でいけるんじゃないか？


## 用途
不良品検出、顔認証など`open set recognition problem`が絡むタスク

## `Fixed AdaCos`
\[
\tilde{s}_f = \sqrt{2} \cdot \log(C - 1)
\]
クラス数が16だとすると
\[
\sqrt{2} \log(16 - 1)
\]
\[
約3.829
\]

